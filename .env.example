# Ollama Configuration (for Qwen3 LLM)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen3:8b

# Whisper Configuration (for audio transcription)
WHISPER_MODEL=base
WHISPER_DEVICE=cpu
WHISPER_COMPUTE_TYPE=int8

# Available Whisper models (in order of speed vs accuracy):
# tiny    - Fastest, least accurate (~39 MB)
# base    - Good balance (~74 MB) - RECOMMENDED
# small   - Better accuracy (~244 MB)
# medium  - High accuracy (~769 MB)
# large-v2 - Best accuracy (~1550 MB)
# large-v3 - Latest, best accuracy (~1550 MB)

# Available devices:
# cpu  - CPU inference (works everywhere)
# cuda - GPU inference (requires NVIDIA GPU + CUDA)

# Available compute types:
# int8     - Fastest, good quality (RECOMMENDED)
# float16  - Better quality, slower
# float32  - Best quality, slowest

# Environment
NODE_ENV=production